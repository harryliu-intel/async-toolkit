\documentclass{article}
\usepackage{graphicx}
\usepackage{epsf}
\usepackage{epstopdf}

\usepackage{floatflt}
\usepackage{fancyhdr}
\usepackage{array}

\usepackage{xeCJK}
%\setCJKmainfont{SimSun}

\usepackage{sectsty}
%\allsectionsfont{\mdseries\sffamily}
\allsectionsfont{\mdseries\sc}

\usepackage{caption}
\captionsetup{margin=2pc,font=small,labelfont=bf}

%%%%%%%%%%%%%%Mika's figure macro%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\listcaption_ins_epsfig#1#2#3#4{
  \begin{figure}[!tbph!]
  \bigskip
  \begin{center}
  \includegraphics[width=#1]{#2}
  \end{center}
  \caption[#4]{#3}\label{fig:#2}
  \bigskip
  \end{figure}
}

\def\ins_epsfig#1#2#3{
  \begin{figure}[!tbph!]
  \bigskip
  \begin{center}
  \includegraphics[width=#1]{#2}
  \end{center}
  \caption{#3}\label{fig:#2}
  \bigskip
  \end{figure}
}

\def\rotins_epsfig_listcaption#1#2#3#4#5{
  \begin{figure}[!tbph!]
  \bigskip
  \begin{center}
  \includegraphics[angle=#4,width=#1]{#2}
  \end{center}
  \caption[#5]{#3}\label{fig:#2}
  \bigskip
  \end{figure}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\oddsidemargin=0.15in
\evensidemargin=0.15in
\topmargin=0.2in
\textwidth=6.2truein


\pagestyle{fancy}
\lhead{\scriptsize\bfseries\sffamily DRAFT---INTEL CONFIDENTIAL---DRAFT}
\chead{}\rhead{\thepage}
\lfoot{}\cfoot{}\rfoot{}
\renewcommand{\headrulewidth}{0pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{How Fast is This Circuit?}
\author{Mika Nystr\"om \\ {\tt mika.nystroem@intel.com} }

\date{August 6, 2023}
\date{\today}

\begin{document}

\maketitle
\parindent=0pt
\parskip=1.5ex

\arraycolsep=1.4pt\def\arraystretch{1.5}

\begin{abstract}
Abstract.
\end{abstract}

\tableofcontents
\listoffigures
\listoftables

\section{A Stochastic Parable}

Once upon a time, in the distant land of Stochastia, there lived the
eccentric but benevolent King Chance and his (exactly) one million
subjects.  One day, the King noticed that his subjects were getting
bored and spending too much time on wasteful pursuits (in particular
they were watching television wrestling), so he decided to give them
some excitement in life.  The Stochastians were commanded to enter a
weekly lottery, where each of them would bet \$1 and receive a payout
according to a normal distribution with mean 1 and variance~1 (if the payout was less than 0, that would be a debit on the
player's account till the next lottery).  Because the King was
benevolent and wealthy, there was no ``house take'': the expected
payout of the lottery was \$1 for every \$1 bet.  The lottery ran
every Sunday until the unfortunate passing of King Chance.

When the King died, his son Chance~II took over, and decided to make
a change to the lottery, to make it more of a life-changing event for
the winners and also remove the possiblity that a player could go
negative.  Chance~II's new lottery was designed so that every week
there was only one winner, who would get the full \$1,000,000 pot, and
all others would get nothing (that week).  In an early example of code
reuse, the Stochastian Lottery Commission re-used the code from the
first-generation lottery: each Sunday, every subject was assigned a
normal deviate from the 1,1-normal distribution that had been used in
the first-generation lottery and that one subject with the highest
payout according to the first-generation rules was given the whole
pot, and the other players were given nothing (and were also not
charged anything further even if their deviates happened to be below
zero).

During the reigns of the two Stochastian kings, a team of scientists
visited from a famous university in California (unfortunately the
records are not dependable enough to tell exactly which university
they were from).  These academics were concerned that perhaps the
kings were not as benevolent and honest as they seemed to be, and that
there was something amiss with the lottery.  They decided to sample
the lottery by buying one thousand lottery tickets, at the cost of
\$1,000, and did so once during the reign of Chance~I and once again
during the reign of Chance~II.  The tickets for the first-generation
lottery showed a payout that was almost exactly as expected, \$1 mean
with a standard deviation of \$1, and the academics announced that
Chance~I was indeed an honest and benevolent king.  But the
second-generation lottery sampling had quite a different result: none
of the 1,000 tickets that the academics sponsored won the lottery, so
their payout was zero, with zero standard deviation.  Accordingly,
they announced that King Chance~II was a fraud, to much consternation
at the State Department in Washington, D.C., which spent much
diplomatic effort salvaging relations with the Kingdom of Stochastia.

\section{Probability Review}

\def\erf{\mathrm{erf}}
\def\erfc{\mathrm{erfc}}
\def\lt{<}
\def\pdf{f}
\def\cdf{F}
\def\bigint{\displaystyle\int}

\subsection{Probability density function}
A brief review of concepts related to a continuous random variable.  A
continuous random variable $X$ is described by its {\em probability
  density function (p.d.f.)\/} $f_X$ with the property that
\begin{equation}
  Pr(a \le x \lt b) = \int_a^b \pdf_X(x) \, dx \quad .
\end{equation}
The p.d.f.\ has a few elementary properties, for example:
\begin{equation}\label{eq:pdfprops}
  \begin{array}{rl}
    \bigint_{-\infty}^{+\infty} \pdf(x) \, dx &= 1 \\
    \forall x : \pdf(x)               & \ge 0
  \end{array}
\end{equation}

The maximum of the p.d.f.\ we call the {\em mode\/} of the distribution; the
mean $\mu$ of the distribution, if it exists, we can compute as
\begin{equation}
  \mu = \int_{-\infty}^{+\infty} x \pdf(x) ,\ dx \quad ,
\end{equation}
and similarly, the variance
\begin{equation}
  \begin{array}{rl}
    \sigma^2_X &= \bigint_{-\infty}^{+\infty} (x-\mu)^2 \pdf(x) \, dx \\
               &= \bigint_{-\infty}^{+\infty} x^2 \pdf(x) \, dx - \mu^2 
  \end{array}
\end{equation}


Questions surrounding the integrability of p.d.f.s form a big and
advanced field of mathematics and are mostly out of the scope of our
discussion.

\def\normal{\mathcal{N}}

\subsection{The normal distribution}
An example of a p.d.f.\ is the normal, or Gaussian, distribution $\normal$
\begin{equation}
  \pdf_{\normal(\mu,\sigma^2)}(x)=
  \normal(\mu,\sigma^2)(x)   =
  {1 \over \sigma \sqrt{2 \pi} } e ^ { -{1 \over 2} \left( { x - \mu \over \sigma } \right) ^2 } \quad .
\end{equation}
We also write $\phi$ for the 0,1-normal distribution:
\begin{equation}
  \phi(x) =
  \pdf_{\normal(0,1)}(x) =
  \normal(0,1)(x)   \quad .
 \end{equation}
Much of what will interest us will specifically {\em not\/} be normally distributed,
but we shall still have occasion to use the properties of the normal distribution as a guide.

\subsection{Cumulative density function}
If we consider the one-sided integral of the p.d.f.\ of $X$, we call
this the {\em cumulative density function (c.d.f.)\/} $\cdf_X$:
\begin{equation}
  \cdf_X(x) = \int_{-\infty}^x \pdf_X(x') \, dx'
\end{equation}
where we use $x'$ for the dummy integration variable for clarity.

\subsubsection{Survival function}
The complement of $\cdf$ we call the {\em survival function\/} $S_X$:
\begin{equation}
  S_X(x) = 1 - \cdf_X(x) = \int_{x}^{+\infty} \pdf_X(x') \, dx'
\end{equation}

\subsubsection{Median}
The half-way point of $\cdf_X(x)$, i.e., the point at which it equals $1/2$, we
call the median and denote $\tilde{X}$:
\begin{equation}
{1 \over 2} = \cdf_X(\tilde{X}) = S_X(\tilde{X})
\end{equation}

\subsubsection{Defects per million (d.p.m.)}\label{sec:dpm}
In manufacturing scenarios, we are often interested in evaluating a particular
design over a required {\em defect rate\/} usually specified in terms of {\em defects per million,} abbreviated {\em d.p.m.}  For example, a d.p.m.~rate of 1000, corresponding to a shipped defect rate of 0.1\%, maybe used to define a point $\tilde{X}_{1000}$ such that
\begin{equation}
  \cdf_X(\tilde{X}_{1000}) = 0.999
\end{equation}
or equivalently and in general let $\tilde{X}_{\rho}$ be defined by
\begin{equation}
  S_X(\tilde{X}_{\rho}) = {\rho \over 10^6} \quad .
\end{equation}


By equation~\ref{eq:pdfprops}, the c.d.f.\ $\cdf$ integrates to 1 over
the number line and is non-decreasing everywhere.

\subsubsection{For the normal distribution}

The c.d.f.\ of the normal distribution can be written in terms of the
non-elementary {\em error function\/}, so that
\begin{equation}
  \cdf_{\normal(\mu,\sigma^2)}(x) = {1 \over 2} + { 1 \over 2 } \erf \left( { x - \mu \over \sigma \sqrt{2} } \right) 
\end{equation}
and also the complement of the error function {\bf is this right???}
\begin{equation}
  \erfc(x) = 1 - \erf(x) 
\end{equation}
can be used to write the survival function of the normal distribution
in a compact and computationally convenient way:
\begin{equation}
  S_{\normal( \mu,\sigma^2)}(x) = { 1 \over 2 } \erfc \left( { x - \mu \over \sigma \sqrt{2} } \right) \quad .
\end{equation}
The c.d.f.\ of the 0,1-normal distribution is often denoted by
\begin{equation}
  \Phi(x) = {1 \over 2} + { 1 \over 2 } \erf \left( { x \over \sqrt{2} } \right) 
\end{equation}


\subsection{P.d.f.\ of a sum of two random variables}
Given two independent random variables $X \sim \pdf_X(x)$ and $Y \sim \pdf_Y(y)$, the sum $Z = X + Y$ is distributed as the convolution $*$ of the two
\begin{equation}
  \pdf_Z(z) = \int_{-\infty}^{\infty}  \pdf_X(x)\pdf_Y(z - x) \, dx = \pdf_X(z) * \pdf_Y(z) \quad .
\end{equation}
Computationally this means that the Fourier transform of $\pdf$ is interesting,
because the convolution becomes a multiplication in Fourier space.

\subsubsection{For the normal distribution}
As a special case if $X$ and $Y$ are normally distributed so that
\begin{equation}
  \begin{array}{rl}
    X \sim \normal(\mu_X,\sigma_X^2) \\
    Y \sim \normal(\mu_Y,\sigma_Y^2)
  \end{array}
\end{equation}
and $Z = X + Y$ then
\begin{equation}
  Y \sim \normal(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2) \quad .
\end{equation}
This formula follows from the fact that the Gaussian is its own
Fourier transform.

\subsubsection{Integer ``multiple'' of a random variable}
If we are given $X \sim \pdf_X(x)$ and we wish to know the distribution of a
sum of $M$ i.i.d.\ instances of $X$, let's say $Z = X_0 + X_1 + \cdots + X_{M-1}$
then we can write this as the $M$-th convolutional power of $\pdf_X$
\begin{equation}
  \pdf_Z(x) = \pdf_X^{*M}(x) \quad .
\end{equation}

\subsection{The maximum of a set of random variables}
Given $X \sim \pdf_X$ and $Y \sim \pdf_Y$, and $Z = \max(X, Y)$, we have that
\begin{equation}
  \cdf_Z(z) = \cdf_X(z) \cdf_Y(z) \quad .
\end{equation}

For the special case $X_0, X_1, \ldots , X_{N-1} \sim \pdf_X$ and $Z = \max(X_0, X_1, \ldots , X_{N-1})$, 
\begin{equation}
  \cdf_Z(z) = \cdf_X^N(z) \quad .
\end{equation}
A convenient measure of the maximum of distributions is the median of the distribution of the maximum $\tilde{z}$ because
\begin{equation}
  \cdf_Z(\tilde{z}) = {1 \over 2}
\end{equation}
so that
\begin{equation}
\tilde{z} = \cdf^{-1}\left( \sqrt[N]{1 \over 2} \right) \quad .
\end{equation}

If in particular $X \sim \normal (0,1)$ then
\begin{equation}
\tilde{z} = \Phi^{-1}\left( \sqrt[N]{1 \over 2} \right)\quad ;
\end{equation}
since for large $N$
\begin{equation}
 \sqrt[N]{1 \over 2}  \approx 1 - {\ln 2 \over N}
\end{equation}
we have that {\bf is this right??? def'n of erfc is suspect}
\begin{equation}
\tilde{z} \approx \sqrt{2}  \, \erfc^{-1} \left( 2 \ln 2 \over N \right) \quad .
\end{equation}

\subsection{The $n$-ball}
The volume $V_n$ and surface area $S_{n-1}$ of the $n$-ball of radius $R$ in $n$-dimensional space are:
\begin{equation}
  V_n(R) = { \pi^{n/2} \over \Gamma \left( {n \over 2} + 1 \right) } 
\end{equation}
and
\begin{equation}
  S_{n-1}(R) = { 2 \pi^{n/2} \over \Gamma \left( n \over 2 \right) } R^{n - 1} \quad . 
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Speed of a Digital Circuit}

A digital circuit consists of a number of parallel {\em speed
paths\/}, each of which consists of a number of sequential {\em
gates.}  The minimum time allowed between clock pulses, that is, the maximum
speed of computation, is determined by the maximum time across the parallel
speed paths; the time taken for one speed path to compute is determined
by the sum of the {\em gate delays\/} along that path.

We introduce a simplified model as follows: the circuit $\mathcal{C}$
consists of $N$ identical speed paths $\mathcal{P}$, each of which is comprised of
an identical sequence of $M$ gates of type $\mathcal{G}$.

Each gate $G$ has a delay $\Delta$ that is stochastic and is a function over
$n$ dimensions, each of which is assumed to be a 0,1-normal random variable:
\begin{equation}
  \Delta : {\bf R}_n \rightarrow {\bf R} : \Delta(X_0, X_1, \ldots , X_{n-1})
\end{equation}
\begin{equation}
  X_0, X_1, \ldots , X_{n-1} \sim \normal (0,1)
\end{equation}
$\Delta$ here is a nonlinear function that is expensive to evaluate
and does not provide derivatives (in practice, we evaluate $\Delta$ by
running a circuit simulator, such as SPICE, on a test circuit description and
performing a simulated physical measurement on the simulated circuit).  We can, however,
assume that the extrema (of interest) of $\Delta$ {\em over\/} a ball
in $n$ dimensions, if they exist, will be found {\em on\/} the ball.  
Furthermore, we know that there exists a maximum radius $R_\mathrm{max}$ for
which $\Delta$ is defined at all points within the $n$-ball of radius
$R_\mathrm{max}$, but $\Delta$ is not necessarily defined outside of this ball
and may have asymptotes to $+\infty$ near the surface of the ball of radius $R_\mathrm{max}$.  Because
$\Delta$ represents the physical delay of a circuit, it is also everywhere bounded
below by $\Delta_\mathrm{min} \ge 0$.

\subsection{Orders of magnitude}
We may expect the following rough ranges for the parameters above,
for problems of interest:
\begin{equation}
  \begin{array}{rl}
    M &= 10\mbox{--}20 \\
    N &= 10^4\mbox{--}10^9 \\
    n &\approx 100
  \end{array}
\end{equation}

\subsection{Explicit formulation}

We have that the circuit delay
\begin{equation}
  \Delta_\mathcal{C} = \max(\Delta_{\mathcal{P}0},\Delta_{\mathcal{P}1}, \ldots , \Delta_{\mathcal{P}N-1})
\end{equation}
where
\begin{equation}
  \Delta_\mathcal{P} = \sum_0^{M-1} \Delta_{\mathcal{G}i} \quad .
\end{equation}

So if we know the p.d.f.\ of the delay of a single gate, $f_\mathcal{G}(\delta)$ as a function of the delay $\delta$, we can write the p.d.f.\ of the path delay as 
\begin{equation}
  f_\mathcal{P}(\delta) = f_\mathcal{G}(\delta)^{*M}
\end{equation}
so that the c.d.f.\ of the path delay is
\begin{equation}
  F_\mathcal{P}(\delta) = \int_0^\delta f_\mathcal{G}(x)^{*M} \, dx
\end{equation}
and the c.d.f.\ of the circuit delay becomes
\begin{equation}
  F_\mathcal{C}(\delta) = F_\mathcal{P}(\delta)^N
\end{equation}
\def\median{\tilde{\delta_\mathcal{C}}}
so that it if we for example wanted to compute the median delay $\median$ of $\mathcal{C}$,
\begin{equation}
  F_\mathcal{C}(\median) = {1 \over 2}
\end{equation}
and
\begin{equation}
F_\mathcal{P}(\median) = \int_0^{\median} f_\mathcal{G}(x)^{*M} \, dx = {2^{-{1 \over N}}} \quad ,
\end{equation}
which equation we are to solve for $\median$.  (Of course, we can use any other percentile, if we are interested in a d.p.m.\ rate as in section~\ref{sec:dpm}.)


      

\section{Acknowledgements}


\begin{thebibliography}{99}

\bibitem{qdi}{Alain~J.~Martin and Mika Nystr\"om.  Asynchronous techniques for system-on-chip design.  {\it Proc. IEEE}, 94(6). 2006.}

\bibitem{razor}{Dan Ernst, et al.  Razor: A Low-Power Pipeline Based on Circuit-Level Timing Speculation.  36th International Symposium on Microarchitecture (MICRO-36).  September 2003.}

\bibitem{newuoas}{Zaikun Zhang.  无导数优化方法的研究.  On derivative-free optimization methods (in Chinese).  Ph.D.\ thesis, Chinese Academy of Sciences. Beijing: 2012.}

\bibitem{newuoas-http}{{\tt https://www.zhangzk.net/docs/talks/20160806-icnaao-newuoas.pdf}}

\bibitem{newuoa}{M.~J.~D.~Powell.  The NEWUOA software for unconstrained optimization without derivatives.  In {\it Nonconvex Optimization and Its Applications\/} (book series), NOIA {\bf 83}.  Springer-Verlag, 2006.}

\bibitem{brent}{R.~P.~Brent.  {\it Algorithms for Minimization Without Derivatives.} Prentice-Hall, 1972.}

\bibitem{bubblerazor}{G.~Zhang and P.~A.~Beerel.  Stochastic analysis of Bubble Razor.  DATE 2014.}

\bibitem{gaussian}{C.~F.~Gauss.  {\it Theoria Motus Corporum Coelestium.}  Hamburg: 1803.}

\bibitem{methusalem}{E.~J.~Gumbel.  Das alter des Methusalem.  {\it Zeitschrift f\"ur schweizerische Statistik und Volkswirtschaft\/}, {\bf 69}, 516--530. Zurich: 1933. }

\bibitem{gumbel}{E.~J.~Gumbel.  Les valeurs extr\^emes des distributions statistiques.  {\it Annales de l'Institut Henri Poincar\'e\/}, {\bf 5 }(2). Paris: 1935. }

  \bibitem{max}{M.~R.~P.~Grossmann.  {\tt https://max.pm/posts/max\_dist/}.  Checked August 5, 2023.}

  \bibitem{spice}{L.~W.~Nagel and D.~O.~Pederson.  {\it SPICE (Simulation Program with Integrated Circuit Emphasis.}  University of California Memorandum ERL-M382.  Berkeley: 1973.}
    
\end{thebibliography}

\end{document}
