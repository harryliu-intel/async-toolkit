\documentclass{article}
\usepackage{graphicx}
\usepackage{epsf}

\usepackage{floatflt}
\usepackage{fancyhdr}
\usepackage{array}

\usepackage{sectsty}
%\allsectionsfont{\mdseries\sffamily}
\allsectionsfont{\mdseries\sc}

\usepackage{caption}
\captionsetup{margin=2pc,font=small,labelfont=bf}

%%%%%%%%%%%%%%Mika's figure macro%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\listcaption_ins_epsfig#1#2#3#4{
  \begin{figure}[!tbph!]
  \bigskip
  \begin{center}
  \includegraphics[width=#1]{#2}
  \end{center}
  \caption[#4]{#3}\label{fig:#2}
  \bigskip
  \end{figure}
}

\def\ins_epsfig#1#2#3{
  \begin{figure}[!tbph!]
  \bigskip
  \begin{center}
  \includegraphics[width=#1]{#2}
  \end{center}
  \caption{#3}\label{fig:#2}
  \bigskip
  \end{figure}
}

\def\rotins_epsfig_listcaption#1#2#3#4#5{
  \begin{figure}[!tbph!]
  \bigskip
  \begin{center}
  \includegraphics[angle=#4,width=#1]{#2}
  \end{center}
  \caption[#5]{#3}\label{fig:#2}
  \bigskip
  \end{figure}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\oddsidemargin=0.15in
\evensidemargin=0.15in
\topmargin=0.2in
\textwidth=6.2truein


\pagestyle{fancy}
\lhead{\scriptsize\bfseries\sffamily DRAFT---INTEL CONFIDENTIAL---DRAFT}
\chead{}\rhead{\thepage}
\lfoot{}\cfoot{}\rfoot{}
\renewcommand{\headrulewidth}{0pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Functional Modeling of Digital Networking Hardware: \\
  Challenges, Opportunities, and Perspectives}
\author{Mika Nystr\"om \\ {\tt mika.nystroem@intel.com} }
%\date{January 22, 2018}
\date{\today}

\begin{document}

\maketitle
\parindent=0pt
\parskip=1.5ex

\arraycolsep=1.4pt\def\arraystretch{1.5}

\begin{abstract}
  Abstract.
\end{abstract}

\tableofcontents
\listoffigures
\listoftables

\section{Introduction}

At the first VLSI conference, held in Pasadena in January of 1979,
Gordon Moore delivered a challenge to the assembled engineers and
academics: build a complex (large) integrated circuit whose complexity
(size) was attributable to functions other than memory.
Moore wrote, ``$\ldots$if the semiconductor industry had a
million-transistor technology like VLSI, I'm not so sure it would know
what to do with it.  Besides products containing memory devices, it
isn't clear what future electronic products that take advantage of
VLSI will be.''\cite{vlsi2}  He identified the problems of interconnection and
product definition as key to be solved to make use of large numbers of
transistors on a single dice, arguing that the increasing design costs
related to larger designs may even keep up with the ability of the
fabs to deliver larger chips, with the exception of memory chips.  He
closed, ``In fact, unless we address and solve these problems, as we
look back on the VLSI era, we may only be able to say, 'Thanks for the
memories.' ''

While certainly we can argue that Moore's direst predictions of an
insurmountable logjam in the design process for large silicon devices
did not come true, to some extent the same problems are with us today
as 40 years ago.  It remains extremely difficult to do anything with a
large piece of silicon but to put either (a)~memories or (b)~some
highly iterated design (e.g., GPUs, Xeon Phi) on it.  Indeed at the same conference where
Moore spoke, Carver Mead elaborated by saying that the future
challenges were to be met either with ``A. Design methodologies to
manage complexity'' or ``B. Architecture of ultra concurrent
machines.''\cite{mead} Iterated-hardware designs  are a
case of B; design methodologies to manage complexity remain a pressing
issue even today.


\section{Context, Motivation, and Goals}

In our team, we feel that Gordon Moore's 1979 challenge to the digital
VLSI industry has only been imperfectly addressed, especially at large
companies such as Intel.  Development costs for new products are
astronomically high, and schedules are only met by making extremely
generous allowances for development time: new products, even those
that are broadly derivative of prior products, take several years from
start of engineering work until they are available in the marketplace.
As a result, planning new products is extremely risky, and programs
that fail often fail spectacularly, are cancelled at a late stage of
development, or retargeted several times, at immense cost both in
repeated development and opportunity costs from being late-to-market.
Finally, reuse of design work in our industry is often abysmally poor:
it often seems to take as long (or possibly longer) to use already
validated RTL as it does to write things from scratch.  In short,
resource (time and manpower) efficiency in digital hardware design is
low.  Even the projects that have been executed well are not always
effective in all areas: manpower, product quality (bugs, performance),
time to design, etc.

We are aware of examples from industry as well as from academia that
show that it is possible to design sophisticated hardware systems
quickly and with small, effective teams\cite{hot21164,hotalta}.  However, these
examples do seem to be the exception and not the rule.

We are interested in exploring why some groups and some approaches
have been more effective at design efficiency than others.  In this
document, we attempt to synthesize a technical strategy that strives
to use ideas that we have found to be useful in the past.  Some of the
ideas are very old (older than any of the authors), whereas some of
the ideas are newer.  Very little we propose is our own original
ideas; rather, the ideas have been tried and tested successfully by
ourselves or others in some conext.  The interactions between the
different ideas can be difficult to manage: sometimes, concepts that
appear to be pedestrian and secondary turn out to be difficult to
escape, for example, because they are built deep into computer-aided
design (CAD) tools used across the industry (and it is {\em
  emphatically not\/} our goal to rewrite every CAD tool in
existence).  As we shall see, experience in our sister field of
software engineering, where the problems of complexity are similar to
ours but clearer and easier to define, will give us a great deal of
guidance.

Our goal here is to describe a path towards streamlining the design
process, starting from the concept of {\em Functional Modeling,} that
is, a high-level software description of the hardware we are building.
At its most rudimentary, a functional model is a software simulation
of the hardware design.  This is clearly not a new idea\cite{soul},
but we have found that the structure that is present in such a
simulator can be exploited to guide a large fraction of the entire
design process---that is, it is profitable to see the functional
modeling effort not as a byproduct of or a minor stepping stone in the
hardware design effort but instead as a scaffolding upon which the
entire hardware design effort can be erected.  Indeed, this idea is to
a large extent present in contemporary commercial hardware-description
languages (HDLs) such as SystemVerilog\cite{sv}, but while these HDLs
in theory can be used as described, it is in practice very difficult
to do so, for a large number of reasons, which we will examine in more
detail on its own, in section~\ref{sec:sv_shortcomings}.  

\section{Structure and Approach}

We start with a historical overview of the general concept of design
complexity in digital systems (software and hardware).  Several
strands of development from software engineering in response to the
so-called software crisis will be shown to be relevant to hardware
design.  

\section{Historical Perspective}

Concerns regarding design complexity in engineered systems are not
unique to the hardware business.  Similar concerns have arisen as
pressing across many engineering industries, starting in the early
20th century.  Especially after the Second World War, the dazzling technologies
developed in a few years of engineering frenzy during that war started
to be applied in civilian life, and the issue of complexity became so prominent
as to be a concern at every level of business and government; the
interplay between complex engineered systems is in some ways part of
the general issues relating to management of complex organizations,
another thread of 20th-century business development.\cite{org-man}

{\em Brief aside.} Our concern here is not to deal directly with
management structure and other management issues, but it is important
to understand that issues relating to team structure, recruiting,
communications within teams, etc., are never far away when we discuss
engineering methodology. at least in a commercial environment.  We won't fully
be able to avoid those issues here, try as we may.  {\em End brief aside.}

\subsection{Software engineering and the Software Crisis}

Perhaps the purest instance of design complexity's becoming
overwhelming was observed in the work of our software colleagues,
namely in the then-nascent field of software engineering.  Electrical
engineers, hardware designers, civil engineers, can always attempt to
shift the blame of difficulties to physical issues: ``the technical
problems we are trying to solve are just so difficult and so novel
that solutions are bound to be a little bit tricky and complex---it's
not our fault.''  In software, this is not so, since the entire design
of a software system is ``synthetic'' (although software people can
and do blame the hardware designers for providing infrastructure that is
difficult to work with).  This situation has been known as ``the
software crisis'' at least since 1968.  Edsger Dijkstra put it plainly
in his 1972 Turing Lecture,

``in those days [1950s] one often encountered the naive expectation that, once
more powerful machines were available, programming would no longer be
a problem, for then the struggle to push the machine to its limits
would no longer be necessary and that was all what programming was
about, wasn’t it? But in the next decades something completely
different happened: more powerful machines became available, not just
an order of magnitude more powerful, even several orders of magnitude
more powerful. But instead of finding ourselves in the state of
eternal bliss of all programming problems solved, we found ourselves
up to our necks in the software crisis! How come?

\ldots

the major cause is... that the machines have become several orders of
magnitude more powerful! To put it quite bluntly: as long as there
were no machines, programming was no problem at all; when we had a few
weak computers, programming became a mild problem, and now we have
gigantic computers, programming had become an equally gigantic
problem. \ldots as the power of available
machines grew by a factor of more than a thousand, society's ambition
to apply these machines grew in proportion, and it was the poor
programmer who found his job in this exploded field of tension between
ends and means. The increased power of the hardware, together with the
perhaps even more dramatic increase in its reliability, made solutions
feasible that the programmer had not dared to dream about a few years
before. And now, a few years later, he had to dream about them and,
even worse, he had to transform such dreams into reality! Is it a
wonder that we found ourselves in a software crisis?''\cite{ewd340}

These developments in software are exactly analogous (replacing the word ``machines'' with ``fabrication technologies'' and ``programmer'' with ``circuit designer'') to another of
Mead's observations at the 1979 conference: ``VLSI is a statement
about system complexity, not about transistor size or circuit
performance.  VLSI defines a technology capable of creating systems so
complicated that coping with the raw complexity overwhelms all other
difficulties.''\cite{mead}  This situation has long since come to pass, but the recognition
that complexity is {\em the\/} problem in system design is still not as
clear for us hardware engineers as it is for the software engineers.  We
conclude that the best way we know to tackle design complexity in hardware
design (at least without inventing completely new technologies) is to apply,
judiciously, techniques from the software world to the hardware design process.

\subsection{The response of software engineering to the software crisis}

Software leaders in the late 1960s and early 1970s responded to the software
crisis in several ways.  The most obvious was a strong emphasis on structured
development of software

\begin{itemize}
\item Structured programming (Dijkstra)\cite{structured}

\item Object-oriented programming (O.-J. Dahl's SIMULA67)\cite{simula}

\item Modular programming (Wirth's Modula)\cite{pascal,modula}

\item Structured concurrent programming (Hoare's monitors\cite{monitors}, as applied in Hansen's Concurrent Pascal\cite{concurrent-pascal})

\item Communicating Sequential Processes\cite{ref:csp}
\end{itemize}

At the same time that these techniques were being developed, a strong effort to
apply mathematical techniques to verify the correctness of the programs being
written was underway.

\begin{itemize}

\item Floyd-Hoare logic (Floyd's application to flowcharts\cite{floyd}, Hoare's to programs\cite{hoare-logic})

\item Lamport logic (generalization of Hoare logic to concurrent programs)\cite{lamport-logic}

\item Invariant-based correctness proofs (e.g., the classic solution by Dekker to the synchronization problem\cite{dekker})

\item Abstract data types (ADTs)\cite{adt-liskov}

\item Temporal logic and model checking\cite{browne,pneuli}
  
\end{itemize}

\subsection{Model checking}

In the  second list, model checking stands out, for two reasons: it is
the only technique on the list that is limited to bounded-size (in
time and space) systems, and it was from the start developed in the
context of hardware verification (probably because of its limitation
to bounded-size systems).  Apart from this, all the listed techniques
were first used in software design and verification before being
applied to hardware design---some of the techniques have never been
fully applied to hardware design.  Nevertheless, in daily practice as
of today (2018) the formal techniques are probably more widely
familiar among hardware practitioners than among software
practitioners.  This is likely because of the perceived higher costs
of failure in hardware design than in software.

Model checking is an instructive example for discussing techniques for
managing design complexity.  There is no question that the tools used
in model checking (SAT solvers\cite{knuth-satisfiability}, BDD packages\cite{ref:bryant}, etc., and commercial
packages such as JasperGold) can achieve results in terms of finding
programming errors in RTL code in ways that were not possible before.
Yet model checking is not at all a panacea: seen over the entirety of
a SoC program, it is rare that the deployment of model checking makes
a significant impact on staffing costs or development schedules
(although one can certainly argue that the presence of model-checking
tools ensures higher-quality end results).  The reason for this is
simple: model checking deals with design problems ``in the small,'' on
the level of a single finite-state machine (FSM).  Because model
checking executes code until it reaches a fixed point, it
fundamentally cannot deal with systems that can have unbounded
behavior.  A hardware designer now objects: all hardware systems are finite,
therefore that should be good enough.  The problem is that even a very small
system can have such complex behavior that it is ``as good as unbounded'' (no
existing system can verify properties across its entire behavior).

As an example, the author has written an example SystemVerilog code that is
parameterized in size.  Since a model checker relies on direct
execution of the code, the size parameter must be bound {\em before\/}
attempting a model check.  With the parameter implying 16 free
variables, JasperGold succeeds in validating the design in a few
seconds.  With the size parameter implying 64 free variables, a
JasperGold run of several days shows no sign of approaching a proof.
Of course, the Verilog code can be proved correct ``on the back of a
napkin''\ldots (See appendix~\ref{apdx:corners}.)

This discussion gets us closer to the problem we are trying to solve.  We must
realize that design complexity is a disease, not a symptom.  We do not
seek to enrich those that peddle design-automation tools to ameliorate the
symptoms of design complexity; instead we must attack the disease of design
complexity directly, by removing the complexity itself.  One of the obvious
challenges with this approach is that it likely leads to customer-visible
specification changes (that is, where a specification is bad, it must be changed,
not adapted to).

If it appears that the problem of late-bound parameters (size
parameters) is what makes model-checking insufficient for an effective
and reusable proof and, perhaps, some better method of checking
program correctness will appear, then the reader has misunderstood the
point of this example.  Model checking is not the enemy, and the
source of the problem is not with the proof technique! As the example
in appendix~\ref{apdx:corners} shows, even a very small and simple
program may have behavior that is in practice indistinguishable from
being unbounded.  (This observation can be credited to Marvin
Minsky in 1967\cite{minsky}.)  No proof technique can on its own
overcome this issue; that the program is correct is, {\it without
  further information,} simply beyond human (or mechanical)
understanding.

Of course, as we know, the example program is easily provable correct.  But it
is provable only because the technique used to generate it is known, and the
design exploits a classic theorem in discrete mathematics.

The fact that the correctness of the example program (or another one
of similar complexity) is on its own beyond mechanical and human
understanding is a simple consequence of what is known in computability
theory as the ``Halting Problem,'' shown by Turing in 1937 to be unsolvable\cite{turing}.

\subsection{The finite and specific size of hardware designs is not a safe haven}

That the halting problem has immediate and practical consequences can
be seen from a study of the so-called ``Busy Beaver'' problem, which
is outside of the scope of this discussion, except as to the fact that
the busy beavers are examples of very simple machines with very
complicated behaviors\cite{busy-beaver}.

{\em Straw man argument.} any piece of hardware is of finite extent (well,
once the size parameters are bound, and we do not care about hardware
with unbound size parameters, because we will only build hardware with
bound sizes [we already take issue with this restriction]).  Therefore
hardware can be validated by considering the entire state space of the
hardware.  In software, we are, in contrast, often interested in having software
that can run on hardware the characteristics of which (the size, in particular) we do
not know.  Therefore, assuming static sizes to the software data structures is less
useful. {\em End straw man argument.}

To look at things from an extreme
perspective, if the known universe were considered to be a computer
useful for a single task, in its history it could have performed no
more than about $10^{120}$ elementary
operations on that task\cite{computational-universe}---a number much smaller than
the number of states available to even a fairly small digital circuit.  Therefore,
the argument that hardware design is somehow different from software design
because of the finiteness of (a specific piece of) hardware, as opposed to the unboundedness (of a piece of portable-to-future-systems software) is futile.

\subsection{Summary}

In summary, then, the issue with proving the correctness of the given program is not
mainly with the proof algorithm but rather that the programmer has neglected to state
the theorem he used when he developed the program.
For a discussion on why programmers are negligent in their program description, see appendix~\ref{apdx:annotations}.

The fact remains that more than eighty years of work in theoretical
computer science shows that methods that attempt to find bugs in
already designed systems presented without design invariants lead to
intractable problems for anything but the most trivial of examples and
that this approach is generally doomed to fail.  Industrial practice
is to concentrate on more or less haphazard approaches to program
testing, which, no matter how sophisticated they appear, can only explore
infinitesimal parts of the state space and are often no better.
Therefore, the cardinal rule is: {\em instead of expending inordinate
  effort on removing bugs from a designed system, expend that effort on
  developing the design in such a way that there are fewer bugs in it in
  the first place.}\cite{ewd209}


\section{Shortcomings of SystemVerilog as a Functional Modeling Environment}\label{sec:sv_shortcomings}

SystemVerilog\cite{sv} is the de-facto standard HDL today in industry.
A large number of CAD tools vendors support the various facilities
available in SystemVerilog (to a greater or lesser extent).  Obviously
CAD tools vendors are well aware of the costs of developing hardware,
and indeed, SystemVerilog today incorporates a large number of
facilities for parameterization of designs, specifying assertions and
other properties, and even for developing sophisticated
object-oriented test benches.

Yet, SystemVerilog is as a design description not part of the solution to high
development costs and slipping schedules: in fact, we would say it is a big part
of the cause.

One of the biggest problems with SystemVerilog is that once design data
has been entered into the SystemVerilog language, it becomes very difficult
to extract it from the design again.  The language is obscenely complex in
definition, and no usable parsers (to our knowledge) are available that are not
directly coupled to a tool vendor's (usually quite expensive) CAD tool.  As
a result, only what the tool vendors have decided is achievable is achievable.
It is extremely difficult to innovate or to simplify in the environment.  Straightforward
ideas, such as cosimulation, become difficult and expensive to implement, requiring
special-purpose tools, special-purpose libraries, special-purpose training, and even
special-purpose personnel.  It becomes completely impossible to pursue a unified
design methodology.

From the larger point of view of the electronics industry, it becomes unclear
who really is to do the innovation in SystemVerilog.  The hardware designers
have the cutting-edge problems, and are the ones that know what the next generation
of hardware might look like.  But they are effectively prevented from extending
the tools in that direction; instead this is the task of the specialized EDA
vendors, who are less interested in next year's cutting edge tools than they are
in next year's bestselling tools.  And in many cases, EDA vendors appear to
have discovered that it is better to sell ongoing treatments than it is to sell
cures.

We are not the first to point out shortcomings in contemporary common
practice\cite{must-change}.

\section{A Brief History of Functional Modeling Frameworks in the Intel Switching Team}

\section{Current Practice of Functional Modeling in the Intel Switching Team}

\section{Consumers of Functional Modeling of the Intel Switching Team}

\section{General Principles of Design}

\section{What Does ``Good Code'' Look Like?}

An important question that engineers and their managers have to
address every day is, ``what does good code look like?''  Some would
say, ``I know it when I see it,'' but similarly to the case in
constitutional law, this approach has its faults.  Probably its major
fault is that implies the existence of a kind of programming
guild---that programming can only learned through apprenticeship.
This runs counter to our objective of articulating clear goals and
directions.

Certainly one aspect of ``good code'' comes from the concept of the
Kolmogorov complexity\cite{cover+thomas} of the code.  Kolmogorov
complexity applies to any string, not just a program, and measures the
amount of information present in that string as the length of the
shortest program that can print the string.  To give some examples:
the Kolmogorov complexity of an entirely random sequence of bits is
(with probability approaching 1) a little more than the length of the
sequence (in bits).  The Kolmogorov complexity of a very long run of
the digits ($n$ digits) of $\pi$ (which looks very random) is very
low: it is the length of a program for finding digits of $\pi$ plus
the length of the specification of the number of digits, which can be
close to zero for some numbers but never more than $\log n$.  The
exact value of Kolmogorov complexity is uncomputable because of the
Halting Problem and the existence of Busy Beavers.  See Cover and
Thomas\cite{cover+thomas} for more detail on this topic.

The application of Kolmogorov complexity to programming is that the programs
written by humans should have Kolmogorov complexity that is not too far from
their length.  Replicated code and coordinated strings should be avoided,
following the old maxim {\em never write the same code twice.}

However, Kolmogorov complexity is not the whole story here.  Many
program-design methodologies involve a number of program fragments
having closely coordinated structure, which implies low Kolmogorov
complexity.  One example is objected-oriented programming, for example
using design patterns\cite{gang-of-4}.  Generally this type of
repeated structure is relatively harmless {\em as long as the
  correspondences can be checked by a compiler.}  It may still be OK
if the check must be deferred to runtime.  What is definitely not OK
is to have a methodology that depends on multiple pieces of strictly
coordinated code where a mistake is not caught through any other
method than human validation (usually program testing).

In other words, in falling order of desirability:
\begin{itemize}

\item A code fragment is written only once.  (Excellent)

\item A code fragment is written multiple times and the correspondence is checked at compile time.  {\em Good}

\item A code fragment is written multiple times and the correspondence is unconditionally checked at runtime.  {\em Fair}

\item A code fragment is written multiple times and the correspondence is conditionally checked at runtime.  {\em Poor}

\item A code fragment is writen multiple times  and the correspondence is checked only when a specific test for the correspondence is deployed.  {\em Unacceptable}

\end{itemize}



\section{Guidelines for System Design}

\section{Hardware and Software Considerations}


\section{Acknowledgements}

Michael Wrighton

\appendix

\section{On Usable Annotations}\label{apdx:annotations}

An exchange from the 1968 NATO report.\cite{crisis}

Dijkstra: I have a point with respect to the fact that people are
willing to write programs and fail to make the documentation
afterwards. I had a student who was orally examined to show that he
could program. He had to program a loop, and programmed the body of it
and had to fill in the Boolean condition used to stop the
repetition. I did not say a thing, and actually saw him, reading,
following line by line with his finger, five times the whole interior
part of his coding. Only then did he decide to fill in the Boolean
condition — and made it wrong. Apparently the poor boy spent ten
minutes to discover what was meant by what he had written down. I then
covered up the whole thing and asked him, what was it supposed to do,
and forced out of him a sentence describing what it had to do,
regardless of how it had been worked out. When this formulation had
been given, then one line of reasoning was sufficient to fill in the
condition. The conclusion is that making the predocumentation at the
proper moment, and using it, will improve the efficiency with which
you construct your whole thing incredibly. One may wonder, if this is
so obvious, why doesn’t it happen? I would suggest that the reason why
many programmers experience the making of predocumentation as an
additional burden, instead of a tool, is that whatever
predocumentation he produces can never be used mechanically. Only if
we provide him with more profitable means, preferably mechanical, for
using predocumentation, only then will the spiritual barrier be
crossed.

Perlis: The point that Dijkstra just made is an extremely important
one, and will probably be one of the major advantages of
conversational languages over non-conversational ones. However, there
is another reason why people don’t do predocumentation: They don’t
have a good language for it since we have no way of writing predicates
describing the state of a computation.

\section{{\tt corners} module}\label{apdx:corners}

{\tt corners} is an easy-to-prove-correct program that is completely
intractable to model-checking tools and techniques.  The proof is, of course,
left as an instructive exercise for the reader.

\begin{verbatim}
module corners
  #(parameter LN = 3)
   (input  logic                             clk,
    input  logic                             rst_n,

    input  logic [LN-1:0]                    i_x,
    input  logic [LN-1:0]                    i_y,
    input  logic                             i_horiz, 
    input  logic                             i_clr,   

    output logic [(1<<LN)-1:0][(1<<LN)-1:0]  o_ok_h,
    output logic [(1<<LN)-1:0][(1<<LN)-1:0]  o_ok_v,
    output logic [(1<<LN)-1:0][(1<<LN)-1:0]  o_cov    
    );

   localparam N=(1<<LN);

   logic [N-1:0][N-1:0] cov_d, cov_q, resval;

   generate
      for (genvar iii=0; iii<N; ++iii) begin : res_o_b
         for (genvar jjj=0; jjj<N; ++jjj) begin : res_i_b
            if (iii==0 & jjj == 0 | iii == N-1 & jjj == N-1)
              assign resval[iii][jjj] = '1; 
            else
              assign resval[iii][jjj] = '0; 
         end
      end
   endgenerate

   assign o_cov = cov_q;

   always_ff @(posedge clk) begin
      cov_q <= (rst_n & ~i_clr) ? cov_d : resval;
   end

   always_comb begin
      cov_d = cov_q;
      cov_d[i_x][i_y] = '1;
      if (i_horiz)
        cov_d[i_x+1][i_y]   = '1;
      else
        cov_d[i_x]  [i_y+1] = '1;
   end

   generate
      for (genvar ii=0; ii<N; ++ii) begin : gen_ok_outer_blk
         for (genvar jj=0; jj<N; ++jj) begin : gen_ok_inner_blk
            if (ii != N-1)
              assign o_ok_h[ii][jj] = ~cov_q[ii][jj] & ~cov_q[ii+1][jj  ];
            else
              assign o_ok_h[ii][jj] = '0;

            if (jj != N-1)
              assign o_ok_v[ii][jj] = ~cov_q[ii][jj] & ~cov_q[ii  ][jj+1];
            else
              assign o_ok_v[ii][jj] = '0;

         end
      end
   endgenerate

   property legal_request;
      (~rst_n                      |
        i_clr                      |
        i_horiz & o_ok_h[i_x][i_y] |
       ~i_horiz & o_ok_v[i_x][i_y] );
   endproperty

   request_legal:
     assume property (@(posedge clk) legal_request)
       else $error("illegal request to cover already covered squares");

   logic [N*N-1:0] x;

   generate
      for (genvar i=0; i<N; ++i) begin : ass_x_blk
         assign x[N*i+N-1 : N*i] = cov_d[i];
      end
   endgenerate

   property fail_property;
      x == '1;
   endproperty

   fail_corners:
     assert property (@(posedge clk) not fail_property)
       else $error("truncated square death pattern entered");

   int weight;
   assign weight = $countones(x);

   property full;
      (weight >= N*N-2);
   endproperty

   full_cover: cover property (@(posedge clk) full);
endmodule
\end{verbatim}

A test bench is not really needed because the {\tt assume} property
{\tt legal\_request} is a specification of the test bench.  A working test
bench is given below for completeness, for those interested.

\begin{verbatim}
module corners_tb #()();
   parameter LN    = 4;
   parameter print = 0;

   localparam N = (1<<LN);

   logic                                clk;
   logic                                rst_n;

   logic [LN-1:0]                       i_x;
   logic [LN-1:0]                       i_y;
   logic                                i_horiz;
   logic                                i_clr;

   logic [(1<<LN)-1:0][(1<<LN)-1:0]     o_ok_h;
   logic [(1<<LN)-1:0][(1<<LN)-1:0]     o_ok_v;
   logic [(1<<LN)-1:0][(1<<LN)-1:0]     o_cov; // debug only

   corners #(.LN(LN)) u_dut(.*);

   initial begin : genclk
      clk = '0;
      while (1) begin
         #(50);
         clk = ~clk;
      end
   end

   initial begin
      rst_n = '0;
      @(posedge clk); @(posedge clk); @(posedge clk); @(posedge clk);
      rst_n = '1;
   end

   logic full;

   assign full = (o_ok_v == '0) & (o_ok_h == '0);

   initial begin
      logic fail;

      while(1) begin
         @(negedge clk);

         i_clr = full;

         if (~i_clr) begin

            int   x, y;
            logic h;

            while (1) begin
               x = $urandom % N;
               y = $urandom % N;
               h = $urandom % 2;

               if ( h & o_ok_h[x][y])    break;
               if (~h & o_ok_v[x][y])    break;
            end
            i_x     = x;
            i_y     = y;
            i_horiz = h;
         end
      end   
   end

   initial begin
      while(print) begin
         @(posedge clk); #1;

         if (i_clr) $write("======================\n");

         $write("\ncount %d pos %d %d %d\n", $countones(o_cov), i_x, i_y, i_horiz);
         for (int j=0; j<N; ++j) begin
            for (int i=0; i<N; ++i)
               $write("%d", o_cov[i][j]);
            $write("\n");
         end
      end
   end
endmodule
\end{verbatim}




\begin{thebibliography}{99}

\bibitem{vlsi2}{Gordon~E.~Moore.  Are we really ready for VLSI$^2$?
  In  Charles~L.~Seitz, ed., {\it Proceedings of the Caltech Conference on Very Large Scale
    Integration.}  Caltech Computer Science Technical Report 3340.
  Pasadena, Calif.: California Institute of Technology, January 1979, pp.~3--14.}

\bibitem{mead}{Carver~A.~Mead.  VLSI and technological innovation.
  In  Charles~L.~Seitz, ed., {\it Proceedings of the Caltech Conference on Very Large Scale
    Integration.}  Caltech Computer Science Technical Report 3340.
  Pasadena, Calif.: California Institute of Technology, January 1979, pp.~15--28.}

\bibitem{rem}{Martin~Rem.  Mathematical aspects of VLSI design.
  In  Charles~L.~Seitz, ed., {\it Proceedings of the Caltech Conference on Very Large Scale
    Integration.}  Caltech Computer Science Technical Report 3340.
  Pasadena, Calif.: California Institute of Technology, January 1979, pp.~55--63.}

\bibitem{spwm3}{G.~Nelson, ed.  {\it Systems Programming with
    Modula-3.}  Prentice-Hall Series in Innovative Technology.
  Prentice-Hall, 1991.}
  
\bibitem{ethernet}{IEEE Std 802.3-2015.  {\it IEEE Standard for
    Ethernet.}  Institute of Electrical and Electronics Engineers,
  2015.}

\bibitem{ieee1588}{IEEE Std 1588-2008.  {\it IEEE Standard for a
    Precision Clock Synchronization Protocol for Networked Measurement
    and Control Systems.}  Institute of Electrical and Electronics
  Engineers, 2008.}

\bibitem{sv}{ANSI/IEEE 1800-2017.  {\it IEEE Standard for
    SystemVerilog-Unified Hardware Design, Specification, and
    Verification Language.}  American National Standards Institute,
  2017.}

\bibitem{dragon}{Alfred~V.~Aho, Ravi Sethi, and Jeffrey~D.~Ullman. {\it Compilers:\ Principles, Techniques, and Tools.}  Reading, Mass.:\ Addison-Wesley, 1986.}

\bibitem{tangram}{K.~van~Berkel, J.~Kessels, M.~Roncken, R.~Saeijs, and F.~Schalij.  The VLSI-programming language Tangram and its translation into handshake circuits.  In {\it Proc. European Conference on Design Automation}, pp.~384--389, 1991.}

\bibitem{ref:bryant}{Randal~E.~Bryant. Graph-Based Algorithms for Boolean Function Manipulation. {\it  IEEE Transactions on Computers}, {\bf C-35}(8), August 1986,
pp.~677--691.}

\bibitem{parallel_program_design}{K.~Mani Chandy and Jayadev Misra.  {\it Parallel Program Design.}  Reading, Mass.:\ Addison-Wesley, 1988.}

\bibitem{dennis} {Jack~B.~Dennis.  Data Flow Supercomputers. {\it Computer},
November 1980, pp.~48--56.  IEEE Computer Society, 1980.}


\bibitem{discipline}{Edsger~W.~Dijkstra.  {\it A Discipline of Programming.}
Englewood Cliffs, N.J.:\ Prentice-Hall, 1976.}

  
\bibitem{ref:csp}{C.~A.~R.~Hoare. Communicating Sequential Processes. {\it
Communications of the ACM}, {\bf 21}(8):666--677, 1978.}


\bibitem{k+r}{Brian~W.~Kernighan and Dennis~M.~Ritchie. {\it The C Programming Language}, second ed.  Englewood Cliffs, N.J.:\ Prentice-Hall, 1988.}

\bibitem{ref:synthesis}{Alain~J.~Martin.  Compiling Communicating
Processes into Delay-insensitive VLSI circuits. {\it Distributed
Computing}, {\bf 1}(4), 1986.}


 
\bibitem{this-and-that} {Alain~J.~Martin.  Synthesis of Asynchronous VLSI
  Circuits. In J.~Staunstrup, ed., {\it Formal Methods for VLSI Design}. North-Holland, 1990.}

\bibitem{bips-tools} {Jeremy Dion and Louis Monier.  Design Tools For BIPS-0.  WRL Technical Note TN-32.  Palo Alto, Calif.: Digital Equipment Corp., 1992.}

\bibitem{chisel}{Jonathan Bachrach, Huy Vo, Brian Richards, Yunsup Lee,
Andrew Waterman, Rimas Avi\v{z}ienis, John Wawrzynek, Krste Asanovi\'{c}.  Chisel: Constructing Hardware in a Scala Embedded Language.  In {\it Proceedings of DAC 2012.}  Association for Computing Machinery, 2012.}


\bibitem{must-change}{Shacham, O., Azizi, O., Wachs, M., Qadeer, W.,
Asgar, Z., Kelley, K., Stevenson, J.,
Solomatnikov, A., Firoozshahian, A., Lee, B.,
Richardson, S., and M., H. Rethinking digital
design: Why design must change. IEEE Micro
(Nov/Dec 2010).}

\bibitem{blue-verilog}{Bluespec Inc. Bluespec$^{\rm TM}$ SystemVerilog Reference
Guide: Description of the Bluespec SystemVerilog
Language and Libraries. Waltham, MA, 2004.}


\bibitem{crete-tutorial}{Alain~J.~Martin, et al.  Full-Day Tutorial
   Caltech Asynchronous Synthesis Tools (CAST) at ASYNC 2004,
   FORTH, Crete, April 19, 2004.}

\bibitem{alain-ifip}{Alain~J.~Martin.  Tomorrow's Digital Hardware
  will be Asynchronous and Verified.  In {\it Proceedings of the IFIP
    (International Federation for Information Processing) Congress
    1992: Information Processing 1992,} Volume~I.  Amsterdam:
  Elsevier, 1992.}

\bibitem{crisis}{Peter Naur and Brian Randell, eds.  {\it Software Engineering,} report on a conference sponsored by the NATO Science Committee, Garmisch, Germany, 7th to 11th October 1968.  Brussels: NATO, 1969.}


\bibitem{structured}{Edsger~W.~Dijkstra.  Notes on Structured Programming.  EWD249.  Technische Hogeschool Eindhoven, Department of Mathematics, Report 70-WSK-03, August 1969--April 1970.}  


\bibitem{org-man}{William~H.~Whyte.  {\it The Organization Man.}  New York:\ Simon~\&~Schuster, 1956.}

\bibitem{ewd340}{Edsger~W.~Dijkstra.  The humble programmer.  EWD340.  1972~Turing~Lecture.  {\it Communications of the ACM,} {\bf 15}(10):859--866, 1972.}

\bibitem{ewd209}{Edsger~W.~Dijkstra.  A constructive approach to the problem of program correctness.  EWD209.  August 1967.  Also in {\it BIT\/} {\bf 8}, pp.~174--186, 1968.}

\bibitem{retiming}{Charles~E.~Leiserson and James~B.~Saxe.  Retiming synchronous circuitry.  {\it Algorithmica,} {\bf 6}(1--6):5--35, June 1991.}

\bibitem{simulink}{{\it Matlab \& Simulink R2015b.} MathWorks, 2015.}


\bibitem{rdl}{Register Description Working Group.  {\it SystemRDL
    v1.0: A specification for a Register Description Language.}  Napa,
  Calif.:\ The SPIRIT Consortium, 2009.  Republished by Accelera, 2012.}

\bibitem{hot21164}{Gregg Bouchard and Pete Bannon.  Design objective of the 0.35-micron Alpha 21164 microprocessor.  In {\it Presentations of the 1996 Hot Chips Symposium,} held at Stanford University, August 18--20, 1996.  IEEE, 1996.}
  
\bibitem{hotalta}{Mike Davies.  One billion packet per second frame processing pipeline. In {\it Presentations of the 2011 Hot Chips Symposium,} held at Stanford University, August 17--19, 2011.  IEEE, 2011.}

\bibitem{soul}{Tracy Kidder.  {\it The Soul of a New Machine.}  Columbus, Ga.:\ Atlantic-Little, Brown, 1981.}

\bibitem{mmm}{Frederick Brooks.  {\it The Mythical Man-Month.}  Boston, Mass.:\ Addison-Wesley, 1975.}
  

\bibitem{browne}{M.C.~Browne, E.M.~Clarke, D.L.~Dill.  Checking the correctness of sequential circuits.  In {\it Proceedings of the 1985 International Conference on Computer Design,} October 1985.  IEEE, 1985.}

\bibitem{pneuli}{A.~Pneuli.  The temporal semantics of concurrent programs.  In {\it 18th Annual Symposium on Foundations of Computer Science.}  1977.}

\bibitem{dekker}{Theodorus~J.~Dekker, related by E.W.~Dijkstra in: Over de sequentialiteit van procesbeschrijvingen (EWD-35) (in Dutch).  Undated, 1962 or 1963.  See also: E.W.~Dijkstra.  Solution of a problem in concurrent programming control.  {\it CACM\/} {\bf 8}(9), September 1965.  Association for Computing Machinery, 1965.}

\bibitem{minsky-book}{Marvin~L.~Minsky.  {\it Computation: Finite and Infinite Machines.}  Englewood Cliffs, N.J.:\ Prentice-Hall, 1967.}

\bibitem{minsky}{Marvin~L.~Minsky.  The unsolvability of the halting problem.  Section 8.2 of {\it Computation: Finite and Infinite Machines.}
  Englewood Cliffs, N.J.:\ Prentice-Hall, 1967.}

\bibitem{turing}{Alan Turing.  On computable numbers, with an application to the Entscheidungsproblem.  In {\it Proceedings of the London Mathematical Society\/}, Series 2 {\bf 42}.  London Mathematical Society, 1936.}

\bibitem{busy-beaver}{Tibor Rad\'{o}.  On non-computable functions.  {\it Bell System Technical Journal,} {\bf 41}(3), May 1962.}

\bibitem{lamport-logic}{Leslie Lamport.  The `Hoare Logic' of concurrent programs.  {\it Acta Informatica.} {\bf 14}(1):21--37, 1980.}

\bibitem{computational-universe}{Seth Lloyd.  Computational capacity of the universe.  {\it Phys.\ Rev.\ Lett.} {\bf 88}, 237901, May 24, 2002.}

\bibitem{knuth-satisfiability}{Donald~E.~Knuth.  Satisfiability.  {\it The Art of Computer Programming,} Volume 4, Fascicle 6.  Boston, Mass.: Pearson Education, 2016.  Corrected reprint, April 2018.}

\bibitem{loh-jasper}{Lawrence Loh.  {\it Upgrade your verification with Jasper!}  Jasper Design Automation, 2013.  (Jasper Design Automation is now part of Cadence Design Systems, Inc.)}

\bibitem{c}{ISO JTC1/SC22/WG14.  ISO/IEC 9899:2011.  Programming Languages~-~C.  International Organization for Standardization, 2011.}

\bibitem{cover+thomas}{Thomas~M.~Cover and Joy~A.~Thomas.  {\it Elements of Information Theory.}  New York, N.Y.:\ John Wiley \& Sons, 1991.}

\bibitem{floyd}{Robert~W.~Floyd.  Assigning meanings to programs.  In
  J.T.~Schwarz, ed., {\it Mathematical Aspects of Computer Science,}
  Proceedings of Symposium on Applied Mathematics {\bf 19}. American
  Mathematical Society, 1967.}

\bibitem{hoare-logic}{C.A.R.~Hoare.  An axiomatic basis for computer programming.  {\it CACM\/} {\bf 12}(10), pp.~576--580.}

\bibitem{pascal}{Nicklaus Wirth}

\bibitem{modula}{Nicklaus Wirth}

\bibitem{simula}{Ole-Johan Dahl}

\bibitem{smalltalk-72}{Alan Kay}

\bibitem{smalltalk-80}{Adele Goldberg}

\bibitem{java}{James Gosling, Bill Joy, Guy Steele, Gilad Bracha, and Alex Buckley.  {\it The Java Language Specification, Java SE 8 Edition.}  Redwood City, Calif.:\ Oracle, 2015.}

\bibitem{scala}{Martin Odersky, Lex Spoon, and Bill Venners.  {\it Programming in Scala,} third edition.  Walnut Creek, Calif.:\ Artima Press, 2016.}

\bibitem{functional-scala}{Paul Chiusano and R\'{u}nar Bjarnson.  {\it Functional Programming in Scala.} Manning Publications, 2014.}

\bibitem{gang-of-4}{Erich Gamma, John Vlissides, Ralph Johnson, and Richard Helm.  {\it Design Patterns: Elements of Reusable Object-Oriented Software.}  Addison-Wesley, 1994.}

\bibitem{hanson-c}{David~R.~Hanson.  {\it C Interfaces and Implementations: Techniques for Creating Reusable Software.}  Addison-Wesley, 1996.}

\bibitem{monitors}{C.A.R.~Hoare.  Monitors: An operating system structuring concept.  {\it CACM,} {\bf 17}(10), pp.~549--557, October 1974.}

\bibitem{concurrent-pascal}{Per Brinch Hansen.  The programming language Concurrent Pascal.  {\it IEEE Transactions on Software Engineering I.} {\bf 2}, June 1975, pp.~199--207.}

\bibitem{hansen-book}{Per Brinch Hansen.  {\it The Architecture of Concurrent Programs.} Englewood Cliffs, N.J.:\ Prentice-Hall, 1977.}

\bibitem{adt-liskov}{Barbara Liskov}
  
\end{thebibliography}

\newpage

\section{About the Authors}

{\bf Mika Nystr\"om} is the author.

\end{document}
